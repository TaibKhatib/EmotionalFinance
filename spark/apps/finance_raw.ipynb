{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2484679-62b9-4448-9734-db61e962cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if using Jupyter + PySpark\n",
    "import os\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages org.apache.hadoop:hadoop-aws:3.3.6,com.amazonaws:aws-java-sdk-bundle:1.12.374 pyspark-shell\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144c82f2-5011-4f87-b800-81885c354480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d856d967-d04e-4dfd-84e4-9dadaf62e563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/12 02:30:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any old session first\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MinioTest\")\n",
    "    .config(\"spark.master\", \"spark://spark-master:7077\")\n",
    "\n",
    "    # S3A core config\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")  # <-- important for HTTP MinIO\n",
    "\n",
    "    # Credentials providers (no AWS v2 class)\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,\"\n",
    "        \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\"\n",
    "    )\n",
    "\n",
    "    # Multipart cleanup: override \"24h\" etc with numeric ms\n",
    "    .config(\"spark.hadoop.fs.s3a.multipart.purge.age\", \"86400000\")      # 24h\n",
    "    .config(\"spark.hadoop.fs.s3a.multipart.purge.interval\", \"3600000\")  # 1h\n",
    "\n",
    "    # --- S3A timeouts (ms) ---\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.request.timeout\", \"60000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"60000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.acquisition.timeout\", \"60000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.idle.time\", \"60000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ttl\", \"300000\")\n",
    "\n",
    "    # --- S3A thread pool / connection pool (ALL numeric) ---\n",
    "    .config(\"spark.hadoop.fs.s3a.threads.max\", \"96\")\n",
    "    .config(\"spark.hadoop.fs.s3a.threads.keepalivetime\", \"60000\")  # <--- key that defaulted to \"60s\"\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"200\")\n",
    "    .config(\"spark.hadoop.fs.s3a.max.total.tasks\", \"1000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.max.metadata.tasks\", \"100\")\n",
    "    .config(\"spark.hadoop.fs.s3a.max.total.connections\", \"200\")\n",
    "\n",
    "    # --- Spark-level timeouts as numbers ---\n",
    "    .config(\"spark.files.fetchTimeout\", \"60000\")\n",
    "    .config(\"spark.network.timeout\", \"600000\")\n",
    "    .config(\"spark.rpc.askTimeout\", \"600000\")\n",
    "\n",
    "    # (plus the timeouts / thread configs you already set)\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f84901-108c-4635-abf7-4b5bc8bf1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:30:15 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 1:====================================================>      (8 + 1) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|year|week|\n",
      "+----+----+\n",
      "|2020|51  |\n",
      "|2020|52  |\n",
      "|2020|53  |\n",
      "|2021|1   |\n",
      "|2021|2   |\n",
      "|2021|3   |\n",
      "|2021|4   |\n",
      "|2021|5   |\n",
      "|2021|6   |\n",
      "|2021|7   |\n",
      "|2021|8   |\n",
      "|2021|9   |\n",
      "|2021|10  |\n",
      "|2021|11  |\n",
      "|2021|12  |\n",
      "|2021|13  |\n",
      "|2021|14  |\n",
      "|2021|15  |\n",
      "|2021|16  |\n",
      "|2021|17  |\n",
      "|2021|18  |\n",
      "|2021|19  |\n",
      "|2021|20  |\n",
      "|2021|21  |\n",
      "|2021|22  |\n",
      "|2021|23  |\n",
      "|2021|24  |\n",
      "|2021|25  |\n",
      "|2021|26  |\n",
      "|2021|27  |\n",
      "|2021|28  |\n",
      "|2021|29  |\n",
      "|2021|30  |\n",
      "|2021|31  |\n",
      "|2021|32  |\n",
      "|2021|33  |\n",
      "|2021|34  |\n",
      "|2021|35  |\n",
      "|2021|36  |\n",
      "|2021|37  |\n",
      "|2021|38  |\n",
      "|2021|39  |\n",
      "|2021|40  |\n",
      "|2021|41  |\n",
      "|2021|42  |\n",
      "|2021|43  |\n",
      "|2021|44  |\n",
      "|2021|45  |\n",
      "|2021|46  |\n",
      "|2021|47  |\n",
      "|2021|48  |\n",
      "|2021|49  |\n",
      "|2021|50  |\n",
      "|2021|51  |\n",
      "|2021|52  |\n",
      "|2022|1   |\n",
      "|2022|2   |\n",
      "|2022|3   |\n",
      "|2022|4   |\n",
      "|2022|5   |\n",
      "|2022|6   |\n",
      "|2022|7   |\n",
      "|2022|8   |\n",
      "|2022|9   |\n",
      "|2022|10  |\n",
      "|2022|11  |\n",
      "|2022|12  |\n",
      "|2022|13  |\n",
      "|2022|14  |\n",
      "|2022|15  |\n",
      "|2022|16  |\n",
      "|2022|17  |\n",
      "|2022|18  |\n",
      "|2022|19  |\n",
      "|2022|20  |\n",
      "|2022|21  |\n",
      "|2022|22  |\n",
      "|2022|23  |\n",
      "|2022|24  |\n",
      "|2022|25  |\n",
      "|2022|26  |\n",
      "|2022|27  |\n",
      "|2022|28  |\n",
      "|2022|29  |\n",
      "|2022|30  |\n",
      "|2022|31  |\n",
      "|2022|32  |\n",
      "|2022|33  |\n",
      "|2022|34  |\n",
      "|2022|35  |\n",
      "|2022|36  |\n",
      "|2022|37  |\n",
      "|2022|38  |\n",
      "|2022|39  |\n",
      "|2022|40  |\n",
      "|2022|41  |\n",
      "|2022|42  |\n",
      "|2022|43  |\n",
      "|2022|44  |\n",
      "|2022|45  |\n",
      "+----+----+\n",
      "only showing top 100 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read.parquet(\"s3a://market-raw/COPN.SW/\")\n",
    "\n",
    "df.select(\"year\", \"week\") \\\n",
    "  .distinct() \\\n",
    "  .orderBy(\"year\", \"week\") \\\n",
    "  .show(100, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a6bda5-8afd-4d68-8e82-f809bb0efde4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- ('COPN.SW', 'Open'): double (nullable = true)\n",
      " |-- ('COPN.SW', 'High'): double (nullable = true)\n",
      " |-- ('COPN.SW', 'Low'): double (nullable = true)\n",
      " |-- ('COPN.SW', 'Close'): double (nullable = true)\n",
      " |-- ('COPN.SW', 'Adj Close'): double (nullable = true)\n",
      " |-- ('COPN.SW', 'Volume'): long (nullable = true)\n",
      " |-- Date: timestamp_ntz (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 1257\n",
      "+-------------------+-------------------+------------------+--------------------+------------------------+---------------------+-------------------+----+----+\n",
      "|('COPN.SW', 'Open')|('COPN.SW', 'High')|('COPN.SW', 'Low')|('COPN.SW', 'Close')|('COPN.SW', 'Adj Close')|('COPN.SW', 'Volume')|Date               |year|week|\n",
      "+-------------------+-------------------+------------------+--------------------+------------------------+---------------------+-------------------+----+----+\n",
      "|48.5               |49.29999923706055  |47.150001525878906|47.20000076293945   |45.45343017578125       |55446                |2025-04-07 00:00:00|2025|15  |\n",
      "|51.900001525878906 |51.900001525878906 |48.25             |48.70000076293945   |46.89792251586914       |24853                |2025-04-08 00:00:00|2025|15  |\n",
      "|47.45000076293945  |47.45000076293945  |44.95000076293945 |45.150001525878906  |43.479286193847656      |21373                |2025-04-09 00:00:00|2025|15  |\n",
      "|50.5               |50.5               |46.20000076293945 |46.599998474121094  |44.87562942504883       |37274                |2025-04-10 00:00:00|2025|15  |\n",
      "|47.099998474121094 |47.099998474121094 |45.349998474121094|46.29999923706055   |44.58673095703125       |18012                |2025-04-11 00:00:00|2025|15  |\n",
      "|61.400001525878906 |61.79999923706055  |60.79999923706055 |61.70000076293945   |55.50205612182617       |7744                 |2021-12-06 00:00:00|2021|49  |\n",
      "|61.5               |63.79999923706055  |61.5              |62.599998474121094  |56.311649322509766      |37087                |2021-12-07 00:00:00|2021|49  |\n",
      "|62.79999923706055  |68.30000305175781  |62.70000076293945 |68.30000305175781   |61.43906784057617       |98905                |2021-12-08 00:00:00|2021|49  |\n",
      "|68.5999984741211   |68.5999984741211   |65.30000305175781 |65.5999984741211    |59.01028823852539       |21311                |2021-12-09 00:00:00|2021|49  |\n",
      "|65.80000305175781  |66.19999694824219  |64.5              |66.19999694824219   |59.55001449584961       |21087                |2021-12-10 00:00:00|2021|49  |\n",
      "|63.099998474121094 |65.19999694824219  |62.70000076293945 |65.19999694824219   |65.19999694824219       |31874                |2025-12-01 00:00:00|2025|49  |\n",
      "|66.30000305175781  |66.30000305175781  |64.69999694824219 |65.5                |65.5                    |12765                |2025-12-02 00:00:00|2025|49  |\n",
      "|77.9000015258789   |81.19999694824219  |75.80000305175781 |78.30000305175781   |78.30000305175781       |276087               |2025-12-03 00:00:00|2025|49  |\n",
      "|78.80000305175781  |89.0               |78.5999984741211  |89.0                |89.0                    |287344               |2025-12-04 00:00:00|2025|49  |\n",
      "|90.9000015258789   |92.5               |89.19999694824219 |91.4000015258789    |91.4000015258789        |108280               |2025-12-05 00:00:00|2025|49  |\n",
      "|47.400001525878906 |49.70000076293945  |46.5              |49.0                |45.86134719848633       |79318                |2023-12-11 00:00:00|2023|50  |\n",
      "|48.599998474121094 |50.79999923706055  |48.0              |50.5                |47.2652702331543        |38675                |2023-12-12 00:00:00|2023|50  |\n",
      "|50.70000076293945  |52.5               |49.54999923706055 |51.900001525878906  |48.575592041015625      |28555                |2023-12-13 00:00:00|2023|50  |\n",
      "|52.20000076293945  |53.79999923706055  |51.20000076293945 |53.400001525878906  |49.979515075683594      |29362                |2023-12-14 00:00:00|2023|50  |\n",
      "|53.599998474121094 |54.20000076293945  |52.70000076293945 |52.70000076293945   |49.32434844970703       |26650                |2023-12-15 00:00:00|2023|50  |\n",
      "+-------------------+-------------------+------------------+--------------------+------------------------+---------------------+-------------------+----+----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# 1) Read ALL Parquet files under COPN.SW (all years, all weeks) into ONE DataFrame\n",
    "prices_all = spark.read.parquet(\"s3a://market-raw/COPN.SW/\")\n",
    "\n",
    "# Spark discovers year and week from folder names (year=YYYY/week=WW). [web:95][web:144]\n",
    "print(\"Schema:\")\n",
    "prices_all.printSchema()\n",
    "\n",
    "print(\"Row count:\", prices_all.count())\n",
    "prices_all.show(20, truncate=False)\n",
    "\n",
    "# 2) (Optional) restrict to 2024â€“2025 inside the same DataFrame\n",
    "prices_24_25 = prices_all.filter(F.col(\"year\").isin(2020, 2021, 2022, 2023, 2024, 2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183a4b3e-2c1b-4cf4-855c-e2a97c8720e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>      (8 + 1) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 1257, columns = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Number of rows\n",
    "n_rows = prices_24_25.count()\n",
    "\n",
    "# Number of columns\n",
    "n_cols = len(prices_24_25.columns)\n",
    "\n",
    "print(f\"rows = {n_rows}, columns = {n_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d0a7f5-b857-4d8c-b238-937a4ebb56ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 5, columns = 7\n"
     ]
    }
   ],
   "source": [
    "prices_sample = spark.read.parquet(\"s3a://market-raw/COPN.SW/year=2025/week=15/\")\n",
    "\n",
    "n_rows = prices_sample.count()\n",
    "\n",
    "# Number of columns\n",
    "n_cols = len(prices_sample.columns)\n",
    "\n",
    "print(f\"rows = {n_rows}, columns = {n_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b21bd2c-6b5d-4616-927d-58606d54a5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- AdjClose: double (nullable = true)\n",
      " |-- Volume: long (nullable = true)\n",
      " |-- Date: timestamp_ntz (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:30:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----------------+-----------------+-----------------+------------------+----------------+-----------------+---------------------+-----+\n",
      "|Date               |Close            |ma5_close        |ma10_close       |ma20_close       |vol5_close        |min5_close      |max5_close       |future_ret           |label|\n",
      "+-------------------+-----------------+-----------------+-----------------+-----------------+------------------+----------------+-----------------+---------------------+-----+\n",
      "|2020-12-14 00:00:00|81.0999984741211 |81.0999984741211 |81.0999984741211 |81.0999984741211 |NULL              |81.0999984741211|81.0999984741211 |0.017262657857208777 |1    |\n",
      "|2020-12-15 00:00:00|82.5             |81.79999923706055|81.79999923706055|81.79999923706055|0.9899505726204885|81.0999984741211|82.5             |-0.03636363636363636 |0    |\n",
      "|2020-12-16 00:00:00|79.5             |81.03333282470703|81.03333282470703|81.03333282470703|1.5011106660099545|79.5            |82.5             |0.031446540880503145 |1    |\n",
      "|2020-12-17 00:00:00|82.0             |81.27499961853027|81.27499961853027|81.27499961853027|1.3175103458239412|79.5            |82.5             |0.009756134777534299 |1    |\n",
      "|2020-12-18 00:00:00|82.80000305175781|81.58000030517579|81.58000030517579|81.58000030517579|1.3292863603400122|79.5            |82.80000305175781|-0.010869583245260515|0    |\n",
      "|2020-12-21 00:00:00|81.9000015258789 |81.74000091552735|81.63333384195964|81.63333384195964|1.3049910879020663|79.5            |82.80000305175781|0.01953600054104423  |1    |\n",
      "|2020-12-22 00:00:00|83.5             |81.94000091552735|81.9000004359654 |81.9000004359654 |1.5109603839082426|79.5            |83.5             |0.025149682324803517 |1    |\n",
      "|2020-12-23 00:00:00|85.5999984741211 |83.16000061035156|82.36250019073486|82.36250019073486|1.5109588438507007|81.9000015258789|85.5999984741211 |-0.02102798428095139 |0    |\n",
      "|2020-12-28 00:00:00|83.80000305175781|83.52000122070312|82.52222273084853|82.52222273084853|1.3736799138595817|81.9000015258789|85.5999984741211 |0.014319772130569343 |1    |\n",
      "|2020-12-29 00:00:00|85.0             |83.96000061035156|82.77000045776367|82.77000045776367|1.4363136610735223|81.9000015258789|85.5999984741211 |0.0                  |0    |\n",
      "+-------------------+-----------------+-----------------+-----------------+-----------------+------------------+----------------+-----------------+---------------------+-----+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Start from your original prices DataFrame\n",
    "df = prices_24_25\n",
    "\n",
    "# 1) Flatten the yfinance-style column names\n",
    "rename_map = {\n",
    "    \"('COPN.SW', 'Open')\"     : \"Open\",\n",
    "    \"('COPN.SW', 'High')\"     : \"High\",\n",
    "    \"('COPN.SW', 'Low')\"      : \"Low\",\n",
    "    \"('COPN.SW', 'Close')\"    : \"Close\",\n",
    "    \"('COPN.SW', 'Adj Close')\": \"AdjClose\",\n",
    "    \"('COPN.SW', 'Volume')\"   : \"Volume\",\n",
    "}\n",
    "\n",
    "for old, new in rename_map.items():\n",
    "    df = df.withColumnRenamed(old, new)\n",
    "\n",
    "df.printSchema()\n",
    "# now we have: Open, High, Low, Close, AdjClose, Volume, Date, year, week\n",
    "\n",
    "# 2) Define windows\n",
    "w5  = Window.orderBy(\"Date\").rowsBetween(-4, 0)    # last 5 rows (including current)\n",
    "w10 = Window.orderBy(\"Date\").rowsBetween(-9, 0)    # last 10 rows\n",
    "w20 = Window.orderBy(\"Date\").rowsBetween(-19, 0)   # last 20 rows\n",
    "w   = Window.orderBy(\"Date\")                       # for next-day label (lead)\n",
    "\n",
    "# 3) Build all rolling features + label on a single DataFrame (df_feats)\n",
    "df_feats = (\n",
    "    df\n",
    "    # moving averages\n",
    "    .withColumn(\"ma5_close\",  F.avg(\"Close\").over(w5))\n",
    "    .withColumn(\"ma10_close\", F.avg(\"Close\").over(w10))\n",
    "    .withColumn(\"ma20_close\", F.avg(\"Close\").over(w20))\n",
    "    # rolling volatility (std dev over 5 days)\n",
    "    .withColumn(\"vol5_close\", F.stddev(\"Close\").over(w5))\n",
    "    # rolling min/max over 5 days\n",
    "    .withColumn(\"min5_close\", F.min(\"Close\").over(w5))\n",
    "    .withColumn(\"max5_close\", F.max(\"Close\").over(w5))\n",
    "    # next day's close and future return\n",
    "    .withColumn(\"next_close\", F.lead(\"Close\", 1).over(w))\n",
    "    .withColumn(\n",
    "        \"future_ret\",\n",
    "        (F.col(\"next_close\") - F.col(\"Close\")) / F.col(\"Close\")\n",
    "    )\n",
    "    # binary label: 1 if next-day return > 0, else 0\n",
    "    .withColumn(\"label\", (F.col(\"future_ret\") > 0).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# 4) Inspect features + label\n",
    "df_feats.select(\n",
    "    \"Date\", \"Close\",\n",
    "    \"ma5_close\", \"ma10_close\", \"ma20_close\",\n",
    "    \"vol5_close\", \"min5_close\", \"max5_close\",\n",
    "    \"future_ret\", \"label\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4f1f01b-7d11-4cc4-97a6-36e3200a60e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 1257, columns = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "n_rows = df_feats.count()\n",
    "\n",
    "# Number of columns\n",
    "n_cols = len(df_feats.columns)\n",
    "\n",
    "print(f\"rows = {n_rows}, columns = {n_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0153439f-50d9-4c12-9f2b-5cdf715e2a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = 1018, test = 239\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df_feats.orderBy(\"Date\")\n",
    "\n",
    "train_df = df.filter(col(\"Date\") < \"2025-01-01\")\n",
    "test_df  = df.filter(col(\"Date\") >= \"2025-01-01\")\n",
    "\n",
    "print(f\"train = {train_df.count()}, test = {test_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8832d014-e719-4f55-9d0d-7208706bc6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:30:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+----------+----------+----------+----------+-----+\n",
      "|Close|ma5_close|ma10_close|ma20_close|vol5_close|min5_close|max5_close|label|\n",
      "+-----+---------+----------+----------+----------+----------+----------+-----+\n",
      "|0    |0        |0         |0         |1         |0         |0         |1    |\n",
      "+-----+---------+----------+----------+----------+----------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:30:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    \"Close\",\n",
    "    \"ma5_close\", \"ma10_close\", \"ma20_close\",\n",
    "    \"vol5_close\", \"min5_close\", \"max5_close\",\n",
    "]\n",
    "\n",
    "df_feats.select([\n",
    "    F.count(F.when(F.col(c).isNull(), 1)).alias(c)\n",
    "    for c in feature_cols + [\"label\"]\n",
    "]).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc5aecf2-25c9-4569-8f38-adadb2e7ed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- AdjClose: double (nullable = true)\n",
      " |-- Volume: long (nullable = true)\n",
      " |-- Date: timestamp_ntz (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- ma5_close: double (nullable = true)\n",
      " |-- ma10_close: double (nullable = true)\n",
      " |-- ma20_close: double (nullable = true)\n",
      " |-- vol5_close: double (nullable = true)\n",
      " |-- min5_close: double (nullable = true)\n",
      " |-- max5_close: double (nullable = true)\n",
      " |-- next_close: double (nullable = true)\n",
      " |-- future_ret: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:34:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:34:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:34:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:34:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+----------------------+\n",
      "|summary|Close             |ma5_close         |ma10_close        |ma20_close        |vol5_close         |min5_close        |max5_close        |future_ret            |\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+----------------------+\n",
      "|count  |1257              |1257              |1257              |1257              |1256               |1257              |1257              |1256                  |\n",
      "|mean   |63.3947891805887  |63.36897241930788 |63.35255082429038 |63.393793962978485|1.045956400429677  |62.09351631109925 |64.63814647252728 |3.577211119074127E-4  |\n",
      "|stddev |13.66947663469863 |13.562062882507979|13.463244685934255|13.399101580100554|0.9328328131486354 |13.458749286685036|13.745919477201925|0.021407539032915646  |\n",
      "|min    |33.849998474121094|34.21999969482422 |34.675            |35.68249988555908 |0.10000038147336439|33.849998474121094|34.650001525878906|-0.12923079270582932  |\n",
      "|25%    |53.400001525878906|53.28000030517578 |53.35999984741211 |53.23749980926514 |0.5683312788420497 |51.900001525878906|54.70000076293945 |-0.011023634062038632 |\n",
      "|50%    |62.70000076293945 |62.58000030517578 |62.54000053405762 |62.36000003814697 |0.8354636598795921 |61.400001525878906|64.0              |-0.0010570663396833931|\n",
      "|75%    |71.5999984741211  |71.68000030517578 |71.80999984741212 |71.91499977111816 |1.2660975620008035 |70.5999984741211  |72.69999694824219 |0.01014488330785779   |\n",
      "|max    |104.5999984741211 |97.4              |91.59000015258789 |90.17999992370605 |14.691595507164822 |91.4000015258789  |104.5999984741211 |0.19541989391996661   |\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "df_feats.printSchema()\n",
    "\n",
    "# Row count\n",
    "print(\"Number of rows:\", df_feats.count())\n",
    "\n",
    "# Quick summary stats for main numeric columns\n",
    "num_cols = [\n",
    "    \"Close\",\n",
    "    \"ma5_close\", \"ma10_close\", \"ma20_close\",\n",
    "    \"vol5_close\", \"min5_close\", \"max5_close\",\n",
    "    \"future_ret\"\n",
    "]\n",
    "\n",
    "df_feats.select(num_cols).summary().show(truncate=False)  # count, mean, stddev, min, quartiles, max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "262f0eae-e5f3-4415-bbb9-daa75dc4180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:30:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:42 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/12/12 02:30:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:47 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/12/12 02:30:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:30:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "feature_cols = [\n",
    "    \"Close\",\n",
    "    \"ma5_close\", \"ma10_close\", \"ma20_close\",\n",
    "    \"vol5_close\", \"min5_close\", \"max5_close\",\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"   # skip rows with nulls in feature columns\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "pred_test = model.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e05ff2d-e8cb-45e0-93ff-9eb65927d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:32:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|null_labels|null_rawPrediction|\n",
      "+-----------+------------------+\n",
      "|          1|                 0|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pred_test.select(\n",
    "    F.count(F.when(col(\"label\").isNull(), 1)).alias(\"null_labels\"),\n",
    "    F.count(F.when(col(\"rawPrediction\").isNull(), 1)).alias(\"null_rawPrediction\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54c7535-ed37-4211-b55d-b01a8575bae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:32:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in pred_test : 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 246:======================================>                  (6 + 3) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in pred_eval: 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:32:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pred_eval = (\n",
    "    pred_test\n",
    "    .filter(col(\"label\").isNotNull())\n",
    "    .filter(col(\"rawPrediction\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"Rows in pred_test :\", pred_test.count())\n",
    "print(\"Rows in pred_eval:\", pred_eval.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37c46532-be45-4d89-88a7-da9f68569019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC-ROC: 0.4798\n",
      "Test AUC-PR : 0.4758\n",
      "Test Accuracy: 0.4916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 02:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/12 02:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "binary_eval = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",   # or \"probability\"\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc_roc = binary_eval.evaluate(pred_eval)\n",
    "\n",
    "binary_eval_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "auc_pr = binary_eval_pr.evaluate(pred_eval)\n",
    "\n",
    "multi_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = multi_eval.evaluate(pred_eval)\n",
    "\n",
    "print(f\"Test AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"Test AUC-PR : {auc_pr:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f87d6-ba25-4f33-aabe-41015ad3f712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
